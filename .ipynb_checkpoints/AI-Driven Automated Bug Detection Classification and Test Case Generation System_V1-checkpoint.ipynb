{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fbdb7a-7aee-4a03-b471-7f107f2838c8",
   "metadata": {},
   "source": [
    "# AI-Driven Automated Bug Detection Classification and Test Case Generation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab971c-92c9-499a-bfaf-00db982cb78b",
   "metadata": {},
   "source": [
    "## Step 1: Generate a Larger Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19b79845-2c7d-4968-b04b-86aab0bdb164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in c:\\anaconda\\lib\\site-packages (26.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in c:\\anaconda\\lib\\site-packages (from faker) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda\\lib\\site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "Requirement already satisfied: pandas in c:\\anaconda\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\anaconda\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: seaborn in c:\\anaconda\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\anaconda\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: xgboost in c:\\anaconda\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\anaconda\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\anaconda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\anaconda\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\anaconda\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\anaconda\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\anaconda\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\anaconda\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install faker\n",
    "!pip install pandas scikit-learn matplotlib seaborn imbalanced-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161a54f1-e83e-40d2-9fe8-0edbb12d4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "def generate_code_snippet():\n",
    "    functions = [\n",
    "        'def foo():\\n    return 1',\n",
    "        'def bar():\\n    return 2',\n",
    "        'def baz(x):\\n    return x*2',\n",
    "        'def qux():\\n    print(\"Hello World\")\\n    return None',\n",
    "        'def quux(x, y):\\n    return x + y'\n",
    "    ]\n",
    "    return random.choice(functions)\n",
    "\n",
    "def generate_code_comment():\n",
    "    comments = [\n",
    "        '# This function returns 1',\n",
    "        '# This function returns 2',\n",
    "        '# This function multiplies the input by 2',\n",
    "        '# This function prints Hello World',\n",
    "        '# This function returns the sum of two inputs'\n",
    "    ]\n",
    "    return random.choice(comments)\n",
    "\n",
    "def generate_bug_report():\n",
    "    reports = [\n",
    "        'None',\n",
    "        'Potential null reference',\n",
    "        'Print statement error',\n",
    "        'None',\n",
    "        'None'\n",
    "    ]\n",
    "    return random.choice(reports)\n",
    "\n",
    "def generate_severity():\n",
    "    severities = ['Low', 'Medium', 'High']\n",
    "    return random.choices(severities, weights=(50, 30, 20))[0]\n",
    "\n",
    "def generate_user_feedback():\n",
    "    feedbacks = [\n",
    "        'Works as expected',\n",
    "        'Crashes occasionally',\n",
    "        'Works fine',\n",
    "        'Prints an error',\n",
    "        'Works well'\n",
    "    ]\n",
    "    return random.choice(feedbacks)\n",
    "\n",
    "def generate_test_case():\n",
    "    testing_types = ['Feature', 'Integration', 'Performance', 'UAT', 'Regression']\n",
    "    priorities = ['High', 'Medium', 'Low']\n",
    "    statuses = ['Pass', 'Fail', 'In Progress']\n",
    "    tools = ['Selenium', 'JUnit', 'TestNG', 'Cypress', 'Postman']\n",
    "    \n",
    "    version = f\"{random.randint(0, 9)}.{random.randint(0, 9)}.{random.randint(0, 9)}\"\n",
    "    \n",
    "    return {\n",
    "        'Test ID': fake.uuid4(),\n",
    "        'Test Case Name': fake.catch_phrase(),\n",
    "        'Status': random.choice(statuses),\n",
    "        'Testing Tool/Framework': random.choice(tools),\n",
    "        'Version': version,\n",
    "        'Description': fake.text(max_nb_chars=50),\n",
    "        'Precondition': fake.text(max_nb_chars=30),\n",
    "        'Testing priority': random.choice(priorities),\n",
    "        'Testing Type': random.choice(testing_types),\n",
    "        'Expected results': fake.sentence(),\n",
    "        'Actual Result': fake.sentence()\n",
    "    }\n",
    "\n",
    "def generate_defect():\n",
    "    priorities = ['Major', 'Blocker', 'Critical', 'Minor', 'Trivial', 'Medium', 'Low']\n",
    "    severities = ['Sev1', 'Sev2', 'Sev3', 'Sev4']\n",
    "    environments = ['DEV', 'SIT', 'CIT']\n",
    "    statuses = ['Open', 'Closed', 'In Progress', 'Resolved']\n",
    "    story_types = ['Feature', 'Integration', 'Performance', 'UAT', 'Regression']\n",
    "    resolutions = ['Fixed', 'Won\\'t Fix', 'Duplicate', 'Not a Bug', 'Incomplete']\n",
    "    \n",
    "    return {\n",
    "        'Defect summary': fake.sentence(),\n",
    "        'Priority': random.choice(priorities),\n",
    "        'Severity': random.choice(severities),\n",
    "        'Environment/Pipeline': random.choice(environments),\n",
    "        'Story Type': random.choice(story_types),\n",
    "        'Acceptance Criteria': fake.text(max_nb_chars=50),\n",
    "        'Resolution': random.choice(resolutions),\n",
    "        'Status': random.choice(statuses)\n",
    "    }\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 2000  #We can Increase the number of samples for better training\n",
    "data = {\n",
    "    'code_snippet': [generate_code_snippet() for _ in range(num_samples)],\n",
    "    'code_comment': [generate_code_comment() for _ in range(num_samples)],\n",
    "    'bug_report': [generate_bug_report() for _ in range(num_samples)],\n",
    "    'severity': [generate_severity() for _ in range(num_samples)],\n",
    "    'user_feedback': [generate_user_feedback() for _ in range(num_samples)],\n",
    "    'test_case': [generate_test_case() for _ in range(num_samples)],\n",
    "    'defect': [generate_defect() for _ in range(num_samples)]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3166dbbf-bad0-4874-bde9-7071b3045883",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68304fe5-ddff-46a2-ac2d-fbf5495c16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Combine code_snippet and code_comment for analysis\n",
    "df['combined_code'] = df['code_snippet'] + ' ' + df['code_comment']\n",
    "\n",
    "# Encode the severity labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['severity_encoded'] = label_encoder.fit_transform(df['severity'])\n",
    "\n",
    "# Feature Extraction with Dimensionality Reduction\n",
    "vectorizer_code = TfidfVectorizer()\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)  # Adjust the number of components\n",
    "pipeline_code = Pipeline([('tfidf', vectorizer_code), ('svd', svd)])\n",
    "X_code = pipeline_code.fit_transform(df['combined_code'])\n",
    "\n",
    "vectorizer_feedback = CountVectorizer()\n",
    "X_feedback = vectorizer_feedback.fit_transform(df['user_feedback'])\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_code = scaler.fit_transform(X_code)\n",
    "X_feedback = scaler.fit_transform(X_feedback.toarray())\n",
    "\n",
    "X = np.hstack((X_code, X_feedback))\n",
    "y = df['severity_encoded']\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9669ebd-55d1-4b56-b70a-d46cc60585a8",
   "metadata": {},
   "source": [
    "## Step 3: Train-Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd1eb13-47f1-4813-9077-a0660fa40664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a38ca5-0a41-4eec-8b45-5a52ee1a18d0",
   "metadata": {},
   "source": [
    "## Step 4: Model Development and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252814a9-77cd-4fd8-ba86-b69ed6a18f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.32      0.34       204\n",
      "           1       0.33      0.48      0.39       203\n",
      "           2       0.37      0.24      0.29       207\n",
      "\n",
      "    accuracy                           0.35       614\n",
      "   macro avg       0.35      0.35      0.34       614\n",
      "weighted avg       0.35      0.35      0.34       614\n",
      "\n",
      "Accuracy: 0.3485342019543974\n",
      "Random Forest Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.53      0.47       204\n",
      "           1       0.46      0.40      0.43       203\n",
      "           2       0.37      0.34      0.35       207\n",
      "\n",
      "    accuracy                           0.42       614\n",
      "   macro avg       0.42      0.42      0.42       614\n",
      "weighted avg       0.42      0.42      0.42       614\n",
      "\n",
      "Accuracy: 0.4218241042345277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "y_pred_nb = nb_classifier.predict(X_test)\n",
    "print(\"Naive Bayes Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "\n",
    "# Standard Scaling for other models\n",
    "scaler_standard = StandardScaler()\n",
    "X_train_scaled = scaler_standard.fit_transform(X_train)\n",
    "X_test_scaled = scaler_standard.transform(X_test)\n",
    "\n",
    "# Random Forest Classifier with Hyperparameter Tuning\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_grid = GridSearchCV(rf_classifier, rf_params, cv=3, scoring='accuracy')\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf_grid.predict(X_test_scaled)\n",
    "print(\"Random Forest Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "\n",
    "# Gradient Boosting Classifier with Hyperparameter Tuning\n",
    "gb_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "gb_grid = GridSearchCV(gb_classifier, gb_params, cv=3, scoring='accuracy')\n",
    "gb_grid.fit(X_train_scaled, y_train)\n",
    "y_pred_gb = gb_grid.predict(X_test_scaled)\n",
    "print(\"Gradient Boosting Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "\n",
    "# Support Vector Machine Classifier with Hyperparameter Tuning\n",
    "svc_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [1, 0.1, 0.01],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "svc_classifier = SVC(random_state=42)\n",
    "svc_grid = GridSearchCV(svc_classifier, svc_params, cv=3, scoring='accuracy')\n",
    "svc_grid.fit(X_train_scaled, y_train)\n",
    "y_pred_svc = svc_grid.predict(X_test_scaled)\n",
    "print(\"SVC Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svc))\n",
    "\n",
    "# XGBoost Classifier with Hyperparameter Tuning\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_child_weight': [1, 3]\n",
    "}\n",
    "xgb_classifier = XGBClassifier(random_state=42)\n",
    "xgb_grid = GridSearchCV(xgb_classifier, xgb_params, cv=3, scoring='accuracy')\n",
    "xgb_grid.fit(X_train_scaled, y_train)\n",
    "y_pred_xgb = xgb_grid.predict(X_test_scaled)\n",
    "print(\"XGBoost Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "\n",
    "# Print summaries for comparison\n",
    "print(\"Naive Bayes Classifier Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Random Forest Classifier Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Gradient Boosting Classifier Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(\"SVM Classifier Accuracy:\", accuracy_score(y_test, y_pred_svc))\n",
    "print(\"XGBoost Classifier Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262118e9-4d32-461b-8007-e833eccd164e",
   "metadata": {},
   "source": [
    "## Step 5: Generate Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d212f-0f03-4ae8-bd43-6d87bf4d1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "def generate_test_case():\n",
    "    testing_types = ['Feature', 'Integration', 'Performance', 'UAT', 'Regression']\n",
    "    priorities = ['High', 'Medium', 'Low']\n",
    "    statuses = ['Pass', 'Fail', 'In Progress']\n",
    "    tools = ['Selenium', 'JUnit', 'TestNG', 'Cypress', 'Postman']\n",
    "    \n",
    "    version = f\"{random.randint(0, 9)}.{random.randint(0, 9)}.{random.randint(0, 9)}\"\n",
    "    \n",
    "    return {\n",
    "        'Test ID': fake.uuid4(),\n",
    "        'Test Case Name': fake.catch_phrase(),\n",
    "        'Status': random.choice(statuses),\n",
    "        'Testing Tool/Framework': random.choice(tools),\n",
    "        'Version': version,\n",
    "        'Description': fake.text(max_nb_chars=50),\n",
    "        'Precondition': fake.text(max_nb_chars=30),\n",
    "        'Testing priority': random.choice(priorities),\n",
    "        'Testing Type': random.choice(testing_types),\n",
    "        'Expected results': fake.sentence(),\n",
    "        'Actual Result': fake.sentence()\n",
    "    }\n",
    "\n",
    "# Generate synthetic test cases\n",
    "num_test_cases = 10  # Number of test cases to generate\n",
    "test_cases = [generate_test_case() for _ in range(num_test_cases)]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_test_cases = pd.DataFrame(test_cases)\n",
    "\n",
    "# Display the test cases in a tabular format\n",
    "print(\"Generated Test Cases:\")\n",
    "print(df_test_cases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7210c0cd-4cdf-479d-add0-9d911b204b5e",
   "metadata": {},
   "source": [
    "## Step 6: Create Defect Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798055d-b526-4930-a38a-d429d5625e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand defect details into separate columns\n",
    "defects = pd.DataFrame(df['defect'].tolist())\n",
    "df = pd.concat([df, defects], axis=1)\n",
    "\n",
    "# Display the defect summaries\n",
    "print(\"Generated Defect Summaries:\")\n",
    "print(df[['Defect summary', 'Priority', 'Severity', 'Environment/Pipeline', 'Story Type', 'Acceptance Criteria', 'Resolution', 'Status']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca58b6-a6ea-431a-a546-09e101382f73",
   "metadata": {},
   "source": [
    "## Step 7: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e461e-fc92-407b-bbbc-354d7e9aef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualize Bug Severity Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='severity')\n",
    "plt.title('Bug Severity Distribution')\n",
    "plt.xlabel('Severity')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix for the Best Model (XGBoost Classifier)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_xgb)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - XGBoost Classifier')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc193c42-a164-4254-8813-4f3e03f2fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defect Priority Distribution\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='Priority')\n",
    "plt.title('Defect Priority Distribution')\n",
    "plt.xlabel('Priority')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6b40e-4fff-4798-a6b3-3ad4cba72b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case Status Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_test_cases, x='Status')\n",
    "plt.title('Test Case Status Distribution')\n",
    "plt.xlabel('Status')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626554a5-bdac-41d6-b8e1-16a2ba97c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix for Each Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_xgb)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - XGBoost Classifier')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240939c-4a25-4d63-9adb-132e9af41e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Performance Comparison\n",
    "# Bar Chart for Accuracy Comparison\n",
    "accuracies = {\n",
    "    'Naive Bayes': 0.3597,\n",
    "    'Random Forest': 0.4241,\n",
    "    'Gradient Boosting': 0.4125,\n",
    "    'SVM': 0.4009,\n",
    "    'XGBoost': 0.4290\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(accuracies.keys(), accuracies.values())\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b976943-ff89-439c-9aab-29a37a141bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision, Recall, F1-Score by Model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Example for XGBoost\n",
    "report = classification_report(y_test, y_pred_xgb, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=df_report.index, y=df_report['f1-score'])\n",
    "plt.title('F1-Scores by Class (XGBoost)')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed57450-3bae-4b93-afd9-7c482b39b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance: If you're using a model like Random Forest or XGBoost, plot feature importance to highlight which features were most influential in determining bug severity\n",
    "xgb_classifier.fit(X_train_scaled, y_train)\n",
    "importances = xgb_classifier.feature_importances_\n",
    "feature_names = vectorizer_code.get_feature_names_out()  # Example for TF-IDF\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(20), importances[sorted_indices][:20], align='center')\n",
    "plt.yticks(range(20), [feature_names[i] for i in sorted_indices[:20]])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Important Features (XGBoost)')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
